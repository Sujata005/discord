{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUM+7oGPIiCxTI1B4bubI7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sujata005/discord/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "rRAibdi30v5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive\")"
      ],
      "metadata": {
        "id": "e--ty9NO06h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Jgy0Xj-T2KyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=next(iter(uploaded.values()))\n",
        "type(data)"
      ],
      "metadata": {
        "id": "NFYsCsTeht9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import csv\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer  # stem down the word for example works,working, work etc\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "intents= json.loads(data.decode())     #converts json file into dictionary\n",
        "\n",
        "\n",
        "def train_model():\n",
        "  words=[]\n",
        "  classes=[]\n",
        "  documents=[]\n",
        "  ignore_letters=['?','!',',','.']\n",
        "\n",
        "  for intent in intents['intents']:\n",
        "      for pattern in intent['patterns']:\n",
        "          word_list=nltk.word_tokenize(pattern)\n",
        "          words.extend(word_list)\n",
        "          documents.append((word_list,intent['tag']))\n",
        "          if intent['tag'] not in classes:\n",
        "              classes.append(intent['tag'])\n",
        "\n",
        "  words=[lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
        "  words=sorted(set(words))\n",
        "\n",
        "  classes=sorted(set(classes))\n",
        "  pickle.dump(words, open('words.pkl','wb'))\n",
        "  pickle.dump(classes, open('classes.pkl','wb'))\n",
        "\n",
        "  training=[]\n",
        "  output_empty=[0]* len(classes)\n",
        "\n",
        "  for document in documents:\n",
        "    bag=[]\n",
        "    word_patterns=document[0]\n",
        "    word_patterns=[lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "    for word in words:\n",
        "      bag.append(1) if word in word_patterns else bag.append(0)\n",
        "\n",
        "    output_row=list(output_empty)\n",
        "    output_row[classes.index(document[1])]=1\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "  random.shuffle(training)\n",
        "  training=np.array(training)\n",
        "\n",
        "  train_x=list(training[:, 0])\n",
        "  train_y=list(training[:, 1])\n",
        "\n",
        "  model=Sequential()\n",
        "  model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "  sgd=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "  hist=model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "  model.save('chatbot_model.hs',hist)\n",
        "  print('done')\n",
        "\n",
        "train_model()\n",
        "\n"
      ],
      "metadata": {
        "id": "Vcfz47Vd0_LO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}